{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Diya Kharel-24152363"
      ],
      "metadata": {
        "id": "ZN1edOdQsoHr"
      },
      "id": "ZN1edOdQsoHr"
    },
    {
      "cell_type": "markdown",
      "id": "039fb7a8",
      "metadata": {
        "id": "039fb7a8"
      },
      "source": [
        "# Mushroom Classification using XGBoost – Simplified MLOps Workflow\n",
        "\n",
        "This notebook demonstrates a streamlined machine learning operations (MLOps) workflow tailored to mushroom classification, leveraging XGBoost as the core algorithm. The process includes thorough experiment logging and monitoring to ensure reproducibility and reliability.\n",
        "\n",
        "## Workflow Stages:\n",
        "1. **Data Ingestion and Quality Checks**\n",
        "2. **Data Preparation and Feature Engineering**\n",
        "3. **Training with XGBoost**\n",
        "4. **Model Assessment and Continuous Tracking**\n",
        "5. **Summary of Outcomes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0241df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0241df",
        "outputId": "8348c070-e9b3-4312-b3a5-b355810e4ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f75694f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f75694f",
        "outputId": "8e76cca0-5b29-48c5-836f-30071a722d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: great-expectations in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: altair<5.0.0,>=4.2.1 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (4.2.2)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (43.0.3)\n",
            "Requirement already satisfied: jinja2>=3 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (4.25.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (3.26.1)\n",
            "Requirement already satisfied: mistune>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from great-expectations) (25.0)\n",
            "Requirement already satisfied: pandas<2.2,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (2.1.4)\n",
            "Requirement already satisfied: posthog>3 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (6.7.4)\n",
            "Requirement already satisfied: pydantic>=1.10.7 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (2.11.7)\n",
            "Requirement already satisfied: pyparsing!=3.2.4,>=2.4 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (2.32.4)\n",
            "Requirement already satisfied: ruamel.yaml>=0.16 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (0.18.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (1.16.1)\n",
            "Requirement already satisfied: tqdm>=4.59.0 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (4.15.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.12/dist-packages (from great-expectations) (5.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from altair<5.0.0,>=4.2.1->great-expectations) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from altair<5.0.0,>=4.2.1->great-expectations) (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.2->great-expectations) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3->great-expectations) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->great-expectations) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->great-expectations) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->great-expectations) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->great-expectations) (0.27.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2,>=1.3.0->great-expectations) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.2,>=1.3.0->great-expectations) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog>3->great-expectations) (1.17.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog>3->great-expectations) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog>3->great-expectations) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.7->great-expectations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.7->great-expectations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.7->great-expectations) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->great-expectations) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->great-expectations) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->great-expectations) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->great-expectations) (2025.8.3)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.12/dist-packages (from ruamel.yaml>=0.16->great-expectations) (0.2.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.2->great-expectations) (2.22)\n"
          ]
        }
      ],
      "source": [
        "%pip install great-expectations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Environment & paths ------------------------------------------------------\n",
        "import os, sys, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure project root is on sys.path (use absolute path to avoid surprises)\n",
        "PROJECT_ROOT = os.path.abspath(\".\")\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "# --- Core libs ----------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Sklearn utilities\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, matthews_corrcoef, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "from scipy.stats import zscore\n",
        "import xgboost as xgb\n",
        "\n",
        "# Great Expectations (optional)\n",
        "try:\n",
        "    import great_expectations as gx\n",
        "    from great_expectations.core import ExpectationSuite\n",
        "    GE_AVAILABLE = True\n",
        "    print(\"Great Expectations detected — data checks can run.\")\n",
        "except Exception:\n",
        "    GE_AVAILABLE = False\n",
        "    print(\"ℹGreat Expectations not installed — skipping data checks.\")\n",
        "\n",
        "# --- MLflow setup -------------------------------------------------------------\n",
        "print(\"Initializing MLflow tracking...\")\n",
        "\n",
        "# where local artifacts will be stored when no server is reachable\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, \"mlflow_artifacts\")\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "REMOTE_URI = \"http://mlflow-server:5001\"\n",
        "LOCAL_URI  = f\"file://{os.path.join(PROJECT_ROOT, 'mlruns')}\"\n",
        "\n",
        "# Prefer remote server; if not reachable, fall back to local file store\n",
        "try:\n",
        "    mlflow.set_tracking_uri(REMOTE_URI)\n",
        "    # lightweight call to confirm connectivity\n",
        "    _ = mlflow.search_experiments(max_results=1)\n",
        "    TRACKING_URI = REMOTE_URI\n",
        "    print(f\"Using remote MLflow server: {REMOTE_URI}\")\n",
        "except Exception as err:\n",
        "    print(f\"Remote MLflow unreachable ({err}). Falling back to local store.\")\n",
        "    mlflow.set_tracking_uri(LOCAL_URI)\n",
        "    TRACKING_URI = LOCAL_URI\n",
        "    print(f\"Using local MLflow store: {LOCAL_URI}\")\n",
        "\n",
        "# Create or fetch an experiment and set it active\n",
        "EXPERIMENT_NAME = \"mushroom_classification_comprehensive_notebook\"\n",
        "try:\n",
        "    exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
        "    if exp is None:\n",
        "        EXPERIMENT_ID = mlflow.create_experiment(\n",
        "            EXPERIMENT_NAME,\n",
        "            artifact_location=os.path.join(ARTIFACTS_DIR, EXPERIMENT_NAME),\n",
        "        )\n",
        "        print(f\" Created experiment: {EXPERIMENT_NAME} (id={EXPERIMENT_ID})\")\n",
        "    else:\n",
        "        EXPERIMENT_ID = exp.experiment_id\n",
        "        print(f\"Found existing experiment: {EXPERIMENT_NAME} (id={EXPERIMENT_ID})\")\n",
        "\n",
        "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "except Exception as err:\n",
        "    print(f\"Could not set experiment ({err}). Falling back to default.\")\n",
        "    EXPERIMENT_NAME = \"Default\"\n",
        "    EXPERIMENT_ID = \"0\"\n",
        "\n",
        "print(\"MLflow configured.\")\n",
        "print(f\"   • Experiment: {EXPERIMENT_NAME}\")\n",
        "print(f\"   • Experiment ID: {EXPERIMENT_ID}\")\n",
        "print(f\"   • Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "print(f\"   • Artifacts dir: {ARTIFACTS_DIR}\")\n"
      ],
      "metadata": {
        "id": "XqS5AEFzsOGQ"
      },
      "id": "XqS5AEFzsOGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40830c97",
      "metadata": {
        "id": "40830c97"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "def validate_dataset(\n",
        "    df,\n",
        "    run_name: str = \"data_quality_check\",\n",
        "    min_rows: int = 101,\n",
        "    min_cols: int = 6,\n",
        "    max_rows: int = 100_000,\n",
        "    class_candidates=(\"class\", \"class_encoded\"),\n",
        "    artifact_dir: str | None = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Lightweight data quality gate with optional MLflow logging.\n",
        "\n",
        "    Checks (default thresholds can be changed):\n",
        "      - has at least `min_rows` rows\n",
        "      - has at least `min_cols` columns\n",
        "      - dataframe is not empty\n",
        "      - contains any of the class columns in `class_candidates`\n",
        "      - row count is <= `max_rows`\n",
        "\n",
        "    Returns a dict with overall pass/fail and per-check results.\n",
        "    Saves simple artifacts (dtypes and summary) if possible.\n",
        "    \"\"\"\n",
        "\n",
        "    # Great Expectations is optional — skip quietly if not available\n",
        "    try:\n",
        "        import great_expectations as ge  # noqa: F401\n",
        "        ge_available = True\n",
        "    except Exception:\n",
        "        ge_available = False\n",
        "\n",
        "    # MLflow is optional — only log if available\n",
        "    try:\n",
        "        import mlflow  # type: ignore\n",
        "        mlflow_available = True\n",
        "    except Exception:\n",
        "        mlflow_available = False\n",
        "\n",
        "    # Prepare checks\n",
        "    checks = {\n",
        "        \"enough_rows\": len(df) >= min_rows,\n",
        "        \"enough_columns\": df.shape[1] >= min_cols,\n",
        "        \"not_empty\": not df.empty,\n",
        "        \"has_class_column\": any(c in df.columns for c in class_candidates),\n",
        "        \"reasonable_size\": len(df) <= max_rows,\n",
        "    }\n",
        "    passed = all(checks.values())\n",
        "\n",
        "    # Pretty console feedback\n",
        "    for name, ok in checks.items():\n",
        "        mark = \"✅\" if ok else \"❌\"\n",
        "        print(f\"{mark} {name}\")\n",
        "\n",
        "    # Metrics to log if MLflow is available\n",
        "    base_metrics = {\n",
        "        \"dq_passed\": int(passed),\n",
        "        \"n_rows\": int(len(df)),\n",
        "        \"n_columns\": int(df.shape[1]),\n",
        "        \"n_missing_total\": int(df.isna().sum().sum()),\n",
        "    }\n",
        "\n",
        "    # Choose artifact directory\n",
        "    if artifact_dir is None:\n",
        "        # temp dir inside CWD to keep behaviour predictable in notebooks\n",
        "        artifact_dir = os.path.join(os.getcwd(), \"ml_artifacts\")\n",
        "    Path(artifact_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Write artifacts to disk\n",
        "    try:\n",
        "        dtypes_path = os.path.join(artifact_dir, \"data_types.txt\")\n",
        "        with open(dtypes_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(df.dtypes.to_string())\n",
        "\n",
        "        summary_path = os.path.join(artifact_dir, \"data_summary.txt\")\n",
        "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            # include='all' so object columns are summarised too\n",
        "            f.write(str(df.describe(include=\"all\", datetime_is_numeric=True)))\n",
        "\n",
        "        missing_by_col_path = os.path.join(artifact_dir, \"missing_by_column.txt\")\n",
        "        with open(missing_by_col_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write((df.isna().sum().sort_values(ascending=False)).to_string())\n",
        "    except Exception as e:\n",
        "        print(f\" Unable to write artifacts: {e}\")\n",
        "\n",
        "    # Optional MLflow logging\n",
        "    if mlflow_available:\n",
        "        try:\n",
        "            with mlflow.start_run(run_name=run_name):\n",
        "                for k, v in base_metrics.items():\n",
        "                    mlflow.log_metric(k, v)\n",
        "                for k, v in checks.items():\n",
        "                    mlflow.log_metric(f\"check_{k}\", int(v))\n",
        "\n",
        "                # Log artifacts if present\n",
        "                for p in (dtypes_path, summary_path, missing_by_col_path):\n",
        "                    if os.path.exists(p):\n",
        "                        mlflow.log_artifact(p)\n",
        "\n",
        "                # Record whether GE was available (informational)\n",
        "                mlflow.log_param(\"great_expectations_available\", ge_available)\n",
        "        except Exception as e:\n",
        "            print(f\"MLflow logging skipped due to error: {e}\")\n",
        "\n",
        "    # Optional Great Expectations hook (placeholder to avoid heavy deps)\n",
        "    # You can extend this to run actual GE expectations if desired.\n",
        "    if ge_available:\n",
        "        print(\"ℹGreat Expectations detected (no expectations suite provided; skipping).\")\n",
        "\n",
        "    return {\n",
        "        \"passed\": passed,\n",
        "        \"checks\": checks,\n",
        "        \"metrics\": base_metrics,\n",
        "        \"artifacts_dir\": artifact_dir,\n",
        "        \"message\": \"PASSED\" if passed else \"FAILED\",\n",
        "    }\n",
        "\n",
        "print(\"Data quality validator ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOCYoZqYEAd4"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def _read_flexible_csv(path: Path):\n",
        "    \"\"\"Try common separators first, then fall back to auto-detect.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # 1) Try ; then , (fast, explicit)\n",
        "    for sep in (\";\", \",\"):\n",
        "        try:\n",
        "            return pd.read_csv(path, sep=sep)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) Fall back to Python engine with delimiter sniffing\n",
        "    try:\n",
        "        return pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to read {path.name}: {e}\") from e\n",
        "\n",
        "\n",
        "# ---------- ETL: EXTRACT ----------\n",
        "print(\"▶ Starting data extraction...\")\n",
        "\n",
        "# Search candidates in priority order\n",
        "raw_dir = Path(\"data/raw\")\n",
        "candidates = [\n",
        "    raw_dir / \"secondary_data.csv\",\n",
        "    raw_dir / \"fraction_of_dataset.csv\",\n",
        "]\n",
        "\n",
        "df = None\n",
        "data_source = None\n",
        "\n",
        "for p in candidates:\n",
        "    if p.exists():\n",
        "        df = _read_flexible_csv(p)\n",
        "        data_source = p.name\n",
        "        print(f\"✓ Loaded {data_source} with shape {tuple(df.shape)}\")\n",
        "        break\n",
        "\n",
        "if df is None:\n",
        "    msg = \"No data files found. Please place a CSV in data/raw/ (e.g., secondary_data.csv).\"\n",
        "    print(msg)\n",
        "    raise FileNotFoundError(msg)\n",
        "\n",
        "# Clean column names: trim, normalise punctuation to underscores, collapse repeats\n",
        "df.columns = (\n",
        "    df.columns\n",
        "      .str.strip()\n",
        "      .str.replace(r\"[^\\w]+\", \"_\", regex=True)\n",
        "      .str.strip(\"_\")\n",
        ")\n",
        "\n",
        "print(f\"✓ Columns after cleaning: {list(df.columns)}\")\n",
        "\n",
        "# Optional MLflow logging (only if MLflow is available)\n",
        "try:\n",
        "    import mlflow  # type: ignore\n",
        "\n",
        "    with mlflow.start_run(run_name=\"extract_raw_data\"):\n",
        "        mlflow.log_param(\"data_source\", data_source)\n",
        "        mlflow.log_metric(\"raw_rows\", int(df.shape[0]))\n",
        "        mlflow.log_metric(\"raw_columns\", int(df.shape[1]))\n",
        "    print(\"✓ Logged extract metrics to MLflow.\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ MLflow logging skipped: {e}\")\n",
        "\n",
        "# Validate data quality (uses the revised function name)\n",
        "try:\n",
        "    results = validate_dataset(df, run_name=\"raw_data_validation\")\n",
        "    print(f\"✅ Data validation: {results['message']}\")\n",
        "except NameError:\n",
        "    # Fallback if the validator isn't in scope\n",
        "    print(\"ℹ️ validate_dataset not found; skipping validation step.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Validation error: {e}\")\n"
      ],
      "id": "FOCYoZqYEAd4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIdswz0pEAd7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Visualise gaps in the dataset ---\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.heatmap(\n",
        "    df.isna(),\n",
        "    cmap=\"viridis\",\n",
        "    cbar=True,\n",
        "    yticklabels=False\n",
        ")\n",
        "plt.title(\"Heatmap of Missing Data\", fontsize=14)\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Rows\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Summary of missing values by column ---\n",
        "missing_counts = df.isna().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_counts)\n"
      ],
      "id": "BIdswz0pEAd7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClyCiEreEAd9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Some palettes to rotate through\n",
        "palettes = [\n",
        "    \"Set1\", \"Set2\", \"Set3\",\n",
        "    \"Paired\", \"Pastel1\", \"Pastel2\",\n",
        "    \"Dark2\", \"tab10\", \"tab20\"\n",
        "]\n",
        "\n",
        "# Identify categorical (string/object) columns\n",
        "cat_columns = df.select_dtypes(include=\"object\").columns\n",
        "\n",
        "# Plot each categorical feature with a different colour scheme\n",
        "for i, feature in enumerate(cat_columns):\n",
        "    palette_choice = palettes[i % len(palettes)]\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.countplot(\n",
        "        data=df,\n",
        "        x=feature,\n",
        "        order=df[feature].value_counts().index,\n",
        "        palette=palette_choice\n",
        "    )\n",
        "    plt.title(f\"Distribution of values in '{feature}'\", fontsize=13)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.xlabel(feature)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "ClyCiEreEAd9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HoWlnEPEAeA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Identify numeric columns (exclude encoded target if present)\n",
        "num_columns = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.drop(\n",
        "    \"class_encoded\", errors=\"ignore\"\n",
        ")\n",
        "\n",
        "# Colour options to rotate through\n",
        "colors = [\n",
        "    \"skyblue\", \"salmon\", \"lightgreen\", \"plum\",\n",
        "    \"gold\", \"teal\", \"coral\", \"khaki\"\n",
        "]\n",
        "\n",
        "# Plot histogram + KDE for each numeric feature\n",
        "for i, feature in enumerate(num_columns):\n",
        "    color_choice = colors[i % len(colors)]\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.histplot(df[feature], kde=True, color=color_choice, bins=30)\n",
        "    plt.title(f\"Distribution of '{feature}'\", fontsize=13)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "_HoWlnEPEAeA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "bjbSsrkJEAeD",
        "outputId": "18ad6491-c58b-442c-da79-02a8408dbef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data transformation...\n",
            "MLflow not available: No module named 'mlflow'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3686526444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0m_ML\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_preprocessing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Log original shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0m_ML\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"original_rows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0m_ML\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"original_columns\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"Starting data transformation...\")\n",
        "\n",
        "# ---------- MLflow (optional) ----------\n",
        "_mlflow_available = False\n",
        "try:\n",
        "    import mlflow  # type: ignore\n",
        "    _mlflow_available = True\n",
        "except Exception as _e:\n",
        "    print(f\"MLflow not available: {_e}\")\n",
        "\n",
        "# Helper: log metrics/params if MLflow is available\n",
        "class _ML:\n",
        "    @staticmethod\n",
        "    def start_run(run_name):\n",
        "        if _mlflow_available:\n",
        "            return mlflow.start_run(run_name=run_name)\n",
        "        # Dummy context manager if MLflow is not available\n",
        "        class _Noop:\n",
        "            def __enter__(self): return None\n",
        "            def __exit__(self, *args): return False\n",
        "        return _Noop()\n",
        "\n",
        "    @staticmethod\n",
        "    def log_metric(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                mlflow.log_metric(k, float(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: failed to log metric {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_param(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                # Ensure stringifiable types\n",
        "                if isinstance(v, (dict, list, tuple)):\n",
        "                    v = json.dumps(v, ensure_ascii=False)\n",
        "                mlflow.log_param(k, str(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: failed to log param {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_artifact(path):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                mlflow.log_artifact(path)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: failed to log artifact {path}: {e}\")\n",
        "\n",
        "# ---------- Encoding / Imputation helpers ----------\n",
        "def sample_impute_categorical(series: pd.Series, rng: np.random.Generator | None = None) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Impute missing values in a categorical series by random sampling from observed non-null values.\n",
        "    Returns the imputed series with original (string) categories.\n",
        "    \"\"\"\n",
        "    s = series.astype(\"string\")\n",
        "    mask = s.isna()\n",
        "    if not mask.any():\n",
        "        return s\n",
        "    non_null = s[~mask]\n",
        "    if non_null.empty:\n",
        "        # If a column is entirely missing, fill with a placeholder\n",
        "        return s.fillna(\"Unknown\")\n",
        "    rng = rng or np.random.default_rng()\n",
        "    sampled = rng.choice(non_null.to_numpy(), size=int(mask.sum()), replace=True)\n",
        "    s.loc[mask] = sampled\n",
        "    return s\n",
        "\n",
        "def label_encode(series: pd.Series) -> tuple[pd.Series, dict]:\n",
        "    \"\"\"\n",
        "    Label-encode a categorical series. Returns encoded integers and mapping dict {label: code}.\n",
        "    \"\"\"\n",
        "    le = LabelEncoder()\n",
        "    encoded = le.fit_transform(series.astype(\"string\"))\n",
        "    mapping = {label: int(code) for code, label in enumerate(le.classes_)}\n",
        "    return pd.Series(encoded, index=series.index), mapping\n",
        "\n",
        "# ---------- Transform pipeline ----------\n",
        "with _ML.start_run(run_name=\"data_preprocessing\"):\n",
        "    # Log original shape\n",
        "    _ML.log_metric(\"original_rows\", df.shape[0])\n",
        "    _ML.log_metric(\"original_columns\", df.shape[1])\n",
        "\n",
        "    # 1) Drop columns with too many missing (as per your list, if present)\n",
        "    columns_to_drop = ['gill_spacing', 'stem_surface', 'stem_root', 'spore_print_color', 'veil_type', 'veil_color']\n",
        "    existing_to_drop = [c for c in columns_to_drop if c in df.columns]\n",
        "    if existing_to_drop:\n",
        "        df = df.drop(columns=existing_to_drop)\n",
        "    _ML.log_param(\"dropped_columns\", existing_to_drop)\n",
        "\n",
        "    # 2) Encode and impute selected categorical columns (if present)\n",
        "    #    - impute missing by sampling from observed values\n",
        "    #    - keep them as strings (post-imputation), as in your original flow\n",
        "    rng = np.random.default_rng()\n",
        "    encoded_columns = []\n",
        "\n",
        "    for col in [\"cap_surface\", \"gill_attachment\", \"ring_type\"]:\n",
        "        if col in df.columns:\n",
        "            # Impute missing with sampling on strings\n",
        "            df[col] = sample_impute_categorical(df[col], rng=rng)\n",
        "            encoded_columns.append(col)\n",
        "    _ML.log_param(\"encoded_columns\", encoded_columns)\n",
        "\n",
        "    # 3) Encode target and boolean columns into new *_encoded columns (if present)\n",
        "    target_bool_cols = [\n",
        "        (\"class\", \"class_encoded\"),\n",
        "        (\"does_bruise_or_bleed\", \"does_bruise_or_bleed_encoded\"),\n",
        "        (\"has_ring\", \"has_ring_encoded\"),\n",
        "    ]\n",
        "    encoding_maps = {}\n",
        "    for src, dst in target_bool_cols:\n",
        "        if src in df.columns:\n",
        "            enc_series, mapping = label_encode(df[src])\n",
        "            df[dst] = enc_series\n",
        "            encoding_maps[dst] = mapping\n",
        "    _ML.log_param(\"encoding_maps\", encoding_maps)\n",
        "\n",
        "    # 4) Handle rare categories by collapsing infrequent labels to \"Other\"\n",
        "    possible_columns = ['habitat', 'stem_color', 'gill_color', 'cap_color', 'cap_shape', 'cap_surface', 'ring_type']\n",
        "    rare_threshold = 1000  # same as your original intent\n",
        "    rare_category_mapping = {}\n",
        "    for col in [c for c in possible_columns if c in df.columns]:\n",
        "        counts = df[col].astype(\"string\").value_counts(dropna=False)\n",
        "        rare_vals = counts[counts < rare_threshold].index.tolist()\n",
        "        if rare_vals:\n",
        "            rare_category_mapping[col] = [str(v) for v in rare_vals]\n",
        "            df[col] = df[col].astype(\"string\").apply(lambda x: \"Other\" if x in rare_vals else x)\n",
        "\n",
        "    _ML.log_param(\"rare_category_mapping\", rare_category_mapping)\n",
        "\n",
        "    # 5) Drop original target/boolean text columns if they existed\n",
        "    drop_after_encoding = [c for c, _ in target_bool_cols if c in df.columns]\n",
        "    if drop_after_encoding:\n",
        "        df = df.drop(columns=drop_after_encoding)\n",
        "\n",
        "    # 6) Log processed shape and missingness\n",
        "    _ML.log_metric(\"processed_rows\", df.shape[0])\n",
        "    _ML.log_metric(\"processed_columns\", df.shape[1])\n",
        "    _ML.log_metric(\"missing_values_after_processing\", int(df.isna().sum().sum()))\n",
        "\n",
        "    # 7) Save processed data\n",
        "    processed_dir = \"data/processed\"\n",
        "    os.makedirs(processed_dir, exist_ok=True)\n",
        "    processed_data_path = os.path.join(processed_dir, \"notebook_processed_data.csv\")\n",
        "    df.to_csv(processed_data_path, index=False)\n",
        "    _ML.log_artifact(processed_data_path)\n",
        "\n",
        "    print(f\"Data preprocessing completed. Final shape: {df.shape}\")\n",
        "\n",
        "# ---------- Validate processed data ----------\n",
        "try:\n",
        "    results = validate_dataset(df, run_name=\"processed_data_validation\")\n",
        "    print(f\"Processed data validation: {results.get('message', 'UNKNOWN')}\")\n",
        "except NameError:\n",
        "    print(\"validate_dataset not found; skipping processed data validation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Validation error: {e}\")\n"
      ],
      "id": "bjbSsrkJEAeD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR_cB4WyEAeF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Define numeric columns to screen for outliers\n",
        "cols_to_check = [\"cap_diameter\", \"stem_height\", \"stem_width\"]\n",
        "\n",
        "# Drop rows where any chosen column has |z-score| >= 2.5\n",
        "mask = np.ones(len(df), dtype=bool)\n",
        "for c in cols_to_check:\n",
        "    if c in df.columns:\n",
        "        z = np.abs(zscore(df[c], nan_policy=\"omit\"))\n",
        "        mask &= (z < 2.5) | np.isnan(z)   # keep NaNs, only filter true outliers\n",
        "\n",
        "df = df[mask].reset_index(drop=True)\n",
        "\n",
        "print(f\"Outlier filtering complete. Remaining shape: {df.shape}\")\n"
      ],
      "id": "JR_cB4WyEAeF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c950ff9a",
      "metadata": {
        "id": "c950ff9a"
      },
      "outputs": [],
      "source": [
        "# Streamlined XGBoost Model - Single Run (no A/B testing)\n",
        "print(f\"XGBoost model trained successfully. Accuracy: {model_results['accuracy']:.4f}\")\n",
        "print(\"Single model approach applied for efficiency.\")\n",
        "print(\"Model is ready for evaluation and deployment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0743fdae",
      "metadata": {
        "id": "0743fdae"
      },
      "outputs": [],
      "source": [
        "# --- Enhanced XGBoost Model Training with Comprehensive MLflow Tracking (no emojis) ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# ---------- Optional MLflow wrappers ----------\n",
        "_mlflow_available = False\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn  # ensure sklearn flavor available\n",
        "    _mlflow_available = True\n",
        "except Exception as _e:\n",
        "    print(f\"MLflow not available: {_e}\")\n",
        "\n",
        "class _ML:\n",
        "    @staticmethod\n",
        "    def start_run(run_name, experiment_id=None, nested=False):\n",
        "        if _mlflow_available:\n",
        "            kwargs = {\"run_name\": run_name, \"nested\": nested}\n",
        "            if experiment_id is not None:\n",
        "                kwargs[\"experiment_id\"] = experiment_id\n",
        "            return mlflow.start_run(**kwargs)\n",
        "        class _Noop:\n",
        "            def __enter__(self): return type(\"obj\", (), {\"info\": type(\"obj\", (), {\"run_id\": \"NO_MLFLOW\"})})()\n",
        "            def __exit__(self, *args): return False\n",
        "        return _Noop()\n",
        "\n",
        "    @staticmethod\n",
        "    def log_param(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                if isinstance(v, (dict, list, tuple)):\n",
        "                    v = json.dumps(v, ensure_ascii=False)\n",
        "                mlflow.log_param(k, str(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: could not log param {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_metric(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                mlflow.log_metric(k, float(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: could not log metric {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_artifact(path):\n",
        "        if _mlflow_available and os.path.exists(path):\n",
        "            try:\n",
        "                mlflow.log_artifact(path)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: could not log artifact {path}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_tracking_uri():\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                return mlflow.get_tracking_uri()\n",
        "            except Exception:\n",
        "                return \"UNKNOWN\"\n",
        "        return \"NO_MLFLOW\"\n",
        "\n",
        "print(\"Starting comprehensive XGBoost model training with MLflow tracking...\")\n",
        "\n",
        "# ---------- Prepare features/target ----------\n",
        "if \"class_encoded\" not in df.columns:\n",
        "    raise KeyError(\"Target column 'class_encoded' not found in dataframe.\")\n",
        "\n",
        "X = df.drop(columns=[\"class_encoded\"])\n",
        "y = df[\"class_encoded\"]\n",
        "\n",
        "print(\"Data prepared:\")\n",
        "print(f\"  Features shape: {X.shape}\")\n",
        "print(f\"  Target shape:   {y.shape}\")\n",
        "\n",
        "try:\n",
        "    target_counts = y.value_counts().to_dict()\n",
        "    print(f\"  Target distribution: {target_counts}\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------- Train/Test split ----------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
        ")\n",
        "print(\"Data split complete:\")\n",
        "print(f\"  Training set: {X_train.shape}\")\n",
        "print(f\"  Test set:     {X_test.shape}\")\n",
        "\n",
        "# ---------- MLflow Parent Run ----------\n",
        "# Use existing variable `experiment_id` if defined, else None.\n",
        "try:\n",
        "    _experiment_id = experiment_id  # will NameError if not defined\n",
        "except NameError:\n",
        "    _experiment_id = None\n",
        "\n",
        "parent_run_name = f\"notebook_comprehensive_xgboost_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "with _ML.start_run(run_name=parent_run_name, experiment_id=_experiment_id) as parent_run:\n",
        "    print(f\"Started MLflow parent run: {getattr(getattr(parent_run, 'info', None), 'run_id', 'NO_MLFLOW')}\")\n",
        "\n",
        "    # Log dataset info\n",
        "    _ML.log_param(\"dataset_source\", \"notebook_comprehensive\")\n",
        "    _ML.log_param(\"model_approach\", \"single_xgboost_comprehensive\")\n",
        "    _ML.log_param(\"total_samples\", len(df))\n",
        "    _ML.log_param(\"total_features\", X.shape[1])\n",
        "    _ML.log_param(\"training_samples\", len(X_train))\n",
        "    _ML.log_param(\"test_samples\", len(X_test))\n",
        "    _ML.log_param(\"target_classes\", len(pd.Series(y).unique()))\n",
        "\n",
        "    # Log a small sample of feature names\n",
        "    _ML.log_param(\"sample_features\", X.columns.tolist()[:20])\n",
        "\n",
        "    print(\"Training XGBoost model...\")\n",
        "\n",
        "    # ---------- Child run for actual training ----------\n",
        "    with _ML.start_run(run_name=\"xgboost_comprehensive_training\", nested=True) as child_run:\n",
        "        print(f\"Started MLflow child run: {getattr(getattr(child_run, 'info', None), 'run_id', 'NO_MLFLOW')}\")\n",
        "\n",
        "        # Model definition (mirrors your parameters)\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            use_label_encoder=False,\n",
        "            eval_metric=\"logloss\",\n",
        "            random_state=42,\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            gamma=0,\n",
        "            min_child_weight=1,\n",
        "            reg_alpha=0,\n",
        "            reg_lambda=1\n",
        "        )\n",
        "\n",
        "        # Log all hyperparameters\n",
        "        for param_name, param_value in xgb_model.get_params().items():\n",
        "            _ML.log_param(f\"xgb_{param_name}\", param_value)\n",
        "        print(\"Hyperparameters logged.\")\n",
        "\n",
        "        # Train\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "        # Probabilities (handle binary vs multiclass)\n",
        "        proba = None\n",
        "        try:\n",
        "            proba = xgb_model.predict_proba(X_test)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Determine metrics according to number of classes\n",
        "        classes = np.unique(y_train)\n",
        "        binary = len(classes) == 2\n",
        "\n",
        "        # Accuracy, precision, recall, F1\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        if binary:\n",
        "            # Determine positive label robustly (prefer label 1 if present)\n",
        "            pos_label = 1 if 1 in classes else classes.max()\n",
        "            precision = precision_score(y_test, y_pred, pos_label=pos_label, zero_division=0)\n",
        "            recall    = recall_score(y_test, y_pred, pos_label=pos_label, zero_division=0)\n",
        "            f1        = f1_score(y_test, y_pred, pos_label=pos_label, zero_division=0)\n",
        "        else:\n",
        "            precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "            recall    = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "            f1        = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "        # MCC (defined for binary and multiclass)\n",
        "        mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "        # AUC\n",
        "        auc = None\n",
        "        if proba is not None:\n",
        "            try:\n",
        "                if binary:\n",
        "                    # Use column corresponding to the positive label\n",
        "                    # Map class index to column\n",
        "                    class_to_col = {c: i for i, c in enumerate(xgb_model.classes_)}\n",
        "                    pos_label = 1 if 1 in classes else classes.max()\n",
        "                    auc = roc_auc_score(y_test, proba[:, class_to_col[pos_label]])\n",
        "                else:\n",
        "                    auc = roc_auc_score(y_test, proba, multi_class=\"ovr\", average=\"weighted\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: AUC could not be computed: {e}\")\n",
        "\n",
        "        # Train accuracy and generalisation gap\n",
        "        y_train_pred = xgb_model.predict(X_train)\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        overfitting_gap = train_accuracy - accuracy\n",
        "\n",
        "        # Log metrics\n",
        "        metrics_dict = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"mcc\": mcc,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"overfitting_gap\": overfitting_gap\n",
        "        }\n",
        "        if auc is not None:\n",
        "            metrics_dict[\"auc\"] = auc\n",
        "\n",
        "        for k, v in metrics_dict.items():\n",
        "            _ML.log_metric(k, v)\n",
        "        print(f\"Logged {len(metrics_dict)} metrics to MLflow.\")\n",
        "\n",
        "        # Training metadata\n",
        "        _ML.log_param(\"training_timestamp\", datetime.now().isoformat())\n",
        "        _ML.log_param(\"notebook_version\", \"comprehensive_v1.0\")\n",
        "        _ML.log_param(\"data_preprocessing\", \"label_encoding_outlier_removal\")\n",
        "\n",
        "        # Feature importance (top 20)\n",
        "        if hasattr(xgb_model, \"feature_importances_\"):\n",
        "            importances = dict(zip(X.columns, xgb_model.feature_importances_))\n",
        "            top20 = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "            for rank, (feat, imp) in enumerate(top20, start=1):\n",
        "                _ML.log_metric(f\"feature_importance_rank_{rank:02d}_{feat}\", float(imp))\n",
        "\n",
        "        # Confusion matrix (labels from actual classes)\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=xgb_model.classes_)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[str(c) for c in xgb_model.classes_],\n",
        "            yticklabels=[str(c) for c in xgb_model.classes_]\n",
        "        )\n",
        "        plt.title(\"XGBoost - Confusion Matrix\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        cm_path = \"comprehensive_confusion_matrix_xgboost.png\"\n",
        "        plt.savefig(cm_path, dpi=300, bbox_inches=\"tight\")\n",
        "        _ML.log_artifact(cm_path)\n",
        "        plt.close()\n",
        "        print(\"Confusion matrix artifact saved and logged (if MLflow available).\")\n",
        "\n",
        "        # Log model with signature and input example (best-effort)\n",
        "        try:\n",
        "            if _mlflow_available:\n",
        "                signature = mlflow.models.infer_signature(X_train, xgb_model.predict_proba(X_train) if proba is not None else xgb_model.predict(X_train))\n",
        "                input_example = X_train.head(3)\n",
        "                mlflow.sklearn.log_model(\n",
        "                    xgb_model,\n",
        "                    \"comprehensive_xgboost_model\",\n",
        "                    signature=signature,\n",
        "                    input_example=input_example,\n",
        "                    registered_model_name=\"mushroom_classifier_comprehensive_xgboost\"\n",
        "                )\n",
        "                print(\"Model logged to MLflow with signature and example.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: could not log model with signature: {e}\")\n",
        "            try:\n",
        "                if _mlflow_available:\n",
        "                    mlflow.sklearn.log_model(xgb_model, \"comprehensive_xgboost_model\")\n",
        "                    print(\"Model logged to MLflow without signature.\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Warning: could not log model at all: {e2}\")\n",
        "\n",
        "        # Store results for later use\n",
        "        model_results = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"mcc\": mcc,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"overfitting_gap\": overfitting_gap\n",
        "        }\n",
        "        if auc is not None:\n",
        "            model_results[\"auc\"] = auc\n",
        "\n",
        "        print(\"\\nComprehensive XGBoost training completed.\")\n",
        "        print(f\"  MLflow Parent Run ID: {getattr(getattr(parent_run, 'info', None), 'run_id', 'NO_MLFLOW')}\")\n",
        "        print(f\"  MLflow Child  Run ID: {getattr(getattr(child_run, 'info', None), 'run_id', 'NO_MLFLOW')}\")\n",
        "        print(\"  Model performance:\")\n",
        "        for metric, value in model_results.items():\n",
        "            print(f\"    {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
        "\n",
        "        print(f\"\\nMLflow tracking URI: {_ML.get_tracking_uri()}\")\n",
        "        try:\n",
        "            print(f\"Experiment name: {experiment_name}\")\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "print(\"Comprehensive XGBoost model training with MLflow tracking completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlO2Q7dVEAeN"
      },
      "outputs": [],
      "source": [
        "# Report final XGBoost training result\n",
        "acc = None\n",
        "try:\n",
        "    acc = float(model_results.get(\"accuracy\")) if isinstance(model_results, dict) else None\n",
        "except Exception:\n",
        "    acc = None\n",
        "\n",
        "if acc is not None:\n",
        "    print(f\"XGBoost model trained. Accuracy: {acc:.4f}\")\n",
        "else:\n",
        "    print(\"XGBoost model trained. Accuracy not available.\")\n",
        "\n",
        "print(\"Model is ready for evaluation and monitoring.\")\n"
      ],
      "id": "jlO2Q7dVEAeN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFuEdXtGEAeO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# Optional MLflow wrappers\n",
        "_mlflow_available = False\n",
        "try:\n",
        "    import mlflow\n",
        "    _mlflow_available = True\n",
        "except Exception as _e:\n",
        "    print(f\"MLflow not available: {_e}\")\n",
        "\n",
        "class _ML:\n",
        "    @staticmethod\n",
        "    def start_run(run_name):\n",
        "        if _mlflow_available:\n",
        "            return mlflow.start_run(run_name=run_name)\n",
        "        class _Noop:\n",
        "            def __enter__(self):\n",
        "                return type(\"obj\", (), {\"info\": type(\"obj\", (), {\"run_id\": \"NO_MLFLOW\"})})()\n",
        "            def __exit__(self, *args): return False\n",
        "        return _Noop()\n",
        "\n",
        "    @staticmethod\n",
        "    def log_metric(k, v):\n",
        "        if _mlflow_available:\n",
        "            try: mlflow.log_metric(k, float(v))\n",
        "            except Exception as e: print(f\"Warning: failed to log metric {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_param(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                if isinstance(v, (dict, list, tuple)):\n",
        "                    v = json.dumps(v, ensure_ascii=False)\n",
        "                mlflow.log_param(k, str(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: failed to log param {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_artifact(path):\n",
        "        if _mlflow_available and os.path.exists(path):\n",
        "            try: mlflow.log_artifact(path)\n",
        "            except Exception as e: print(f\"Warning: failed to log artifact {path}: {e}\")\n",
        "\n",
        "\n",
        "def evaluate_xgboost_model_final(model, X_train, y_train, X_test, y_test,\n",
        "                                 overfit_threshold: float = 0.10,\n",
        "                                 high_conf_p: float = 0.80,\n",
        "                                 deploy_thresh: dict | None = None):\n",
        "    \"\"\"\n",
        "    Final XGBoost evaluation without model comparison.\n",
        "    Handles binary and multiclass cases, logs to MLflow if available, and saves artifacts.\n",
        "    \"\"\"\n",
        "    if deploy_thresh is None:\n",
        "        deploy_thresh = {\n",
        "            \"accuracy\": 0.95,\n",
        "            \"precision\": 0.95,\n",
        "            \"recall\": 0.95\n",
        "        }\n",
        "\n",
        "    with _ML.start_run(run_name=\"final_xgboost_evaluation\"):\n",
        "        # Predictions\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        # Probabilities (if available)\n",
        "        proba = None\n",
        "        try:\n",
        "            proba = model.predict_proba(X_test)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Binary vs multiclass handling\n",
        "        classes = np.unique(y_train)\n",
        "        binary = len(classes) == 2\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred_test)\n",
        "        if binary:\n",
        "            # Choose a positive label robustly (prefer 1 if present)\n",
        "            pos_label = 1 if 1 in classes else classes.max()\n",
        "            precision = precision_score(y_test, y_pred_test, pos_label=pos_label, zero_division=0)\n",
        "            recall    = recall_score(y_test, y_pred_test, pos_label=pos_label, zero_division=0)\n",
        "            f1        = f1_score(y_test, y_pred_test, pos_label=pos_label, zero_division=0)\n",
        "        else:\n",
        "            precision = precision_score(y_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
        "            recall    = recall_score(y_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
        "            f1        = f1_score(y_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
        "\n",
        "        mcc = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "        auc = None\n",
        "        if proba is not None:\n",
        "            try:\n",
        "                if binary:\n",
        "                    # map class->column index\n",
        "                    class_to_col = {c: i for i, c in enumerate(model.classes_)}\n",
        "                    pos_label = 1 if 1 in classes else classes.max()\n",
        "                    auc = roc_auc_score(y_test, proba[:, class_to_col[pos_label]])\n",
        "                else:\n",
        "                    auc = roc_auc_score(y_test, proba, multi_class=\"ovr\", average=\"weighted\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: AUC not computed: {e}\")\n",
        "\n",
        "        # Training accuracy and overfitting gap\n",
        "        train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "        overfitting_gap = train_accuracy - accuracy\n",
        "\n",
        "        # Collect and log metrics\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"mcc\": mcc,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"overfitting_gap\": overfitting_gap\n",
        "        }\n",
        "        if auc is not None:\n",
        "            metrics[\"auc\"] = auc\n",
        "\n",
        "        for k, v in metrics.items():\n",
        "            _ML.log_metric(k, v)\n",
        "\n",
        "        # Overfitting message\n",
        "        if overfitting_gap > overfit_threshold:\n",
        "            _ML.log_param(\"overfitting_warning\", f\"gap>{overfit_threshold}\")\n",
        "            print(f\"Potential overfitting detected (gap: {overfitting_gap:.3f})\")\n",
        "        else:\n",
        "            print(f\"Generalisation gap acceptable (gap: {overfitting_gap:.3f})\")\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n--- Final XGBoost Results ---\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"{k.replace('_', ' ').title()}: {v:.4f}\")\n",
        "\n",
        "        # Confusion matrices (counts and row-normalised)\n",
        "        labels = getattr(model, \"classes_\", np.unique(y_test))\n",
        "        cm = confusion_matrix(y_test, y_pred_test, labels=labels)\n",
        "        cm_row = cm.astype(float) / np.maximum(cm.sum(axis=1, keepdims=True), 1.0)  # avoid div-by-zero\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                    xticklabels=[str(c) for c in labels],\n",
        "                    yticklabels=[str(c) for c in labels],\n",
        "                    ax=ax1)\n",
        "        ax1.set_title(\"Confusion Matrix (Counts)\")\n",
        "        ax1.set_xlabel(\"Predicted\")\n",
        "        ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "        sns.heatmap(cm_row * 100.0, annot=True, fmt=\".1f\", cmap=\"Blues\",\n",
        "                    xticklabels=[str(c) for c in labels],\n",
        "                    yticklabels=[str(c) for c in labels],\n",
        "                    ax=ax2)\n",
        "        ax2.set_title(\"Confusion Matrix (Row %)\")\n",
        "        ax2.set_xlabel(\"Predicted\")\n",
        "        ax2.set_ylabel(\"Actual\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        os.makedirs(\"plots\", exist_ok=True)\n",
        "        plot_path = \"plots/final_confusion_matrix_xgboost.png\"\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
        "        _ML.log_artifact(plot_path)\n",
        "        plt.show()\n",
        "\n",
        "        # Feature importance (top 20)\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            fi = pd.DataFrame({\n",
        "                \"feature\": X_test.columns,\n",
        "                \"importance\": model.feature_importances_\n",
        "            }).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "            top = fi.head(20)\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.barh(range(len(top)), top[\"importance\"])\n",
        "            plt.yticks(range(len(top)), top[\"feature\"])\n",
        "            plt.xlabel(\"Feature Importance\")\n",
        "            plt.title(\"Top 20 Feature Importances - XGBoost Model\")\n",
        "            plt.gca().invert_yaxis()\n",
        "            fi_path = \"plots/final_feature_importance_xgboost.png\"\n",
        "            plt.savefig(fi_path, dpi=300, bbox_inches=\"tight\")\n",
        "            _ML.log_artifact(fi_path)\n",
        "            plt.show()\n",
        "\n",
        "        # Deployment readiness checks\n",
        "        # High-confidence predictions metric applies only for binary with proba\n",
        "        high_conf_ok = False\n",
        "        if binary and proba is not None:\n",
        "            # choose positive label probability column consistently\n",
        "            class_to_col = {c: i for i, c in enumerate(model.classes_)}\n",
        "            pos_label = 1 if 1 in classes else classes.max()\n",
        "            pos_idx = class_to_col[pos_label]\n",
        "            y_pred_proba = proba[:, pos_idx]\n",
        "            high_conf_ok = ((y_pred_proba > high_conf_p).sum() + (y_pred_proba < (1 - high_conf_p)).sum()) > (len(y_pred_proba) * 0.8)\n",
        "\n",
        "        deployment_readiness = {\n",
        "            \"accuracy_threshold\": accuracy > deploy_thresh[\"accuracy\"],\n",
        "            \"precision_threshold\": precision > deploy_thresh[\"precision\"],\n",
        "            \"recall_threshold\": recall > deploy_thresh[\"recall\"],\n",
        "            \"no_severe_overfitting\": overfitting_gap < overfit_threshold,\n",
        "            \"high_confidence_predictions\": high_conf_ok if binary and proba is not None else False\n",
        "        }\n",
        "\n",
        "        deployment_ready = all(deployment_readiness.values())\n",
        "        _ML.log_param(\"deployment_ready\", deployment_ready)\n",
        "        for check, passed in deployment_readiness.items():\n",
        "            _ML.log_metric(f\"deployment_check_{check}\", int(passed))\n",
        "            status = \"PASS\" if passed else \"FAIL\"\n",
        "            print(f\"{status} - {check.replace('_', ' ').title()}: {passed}\")\n",
        "\n",
        "        print(f\"\\nModel Deployment Ready: {deployment_ready}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Example call (keep your variables as is)\n",
        "print(\"Starting final XGBoost evaluation...\")\n",
        "final_metrics = evaluate_xgboost_model_final(xgb_model, X_train, y_train, X_test, y_test)\n",
        "print(\"Final XGBoost evaluation completed.\")\n"
      ],
      "id": "cFuEdXtGEAeO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYd0_09DEAeP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# ---------- Optional MLflow wrappers ----------\n",
        "_mlflow_available = False\n",
        "try:\n",
        "    import mlflow\n",
        "    _mlflow_available = True\n",
        "except Exception as _e:\n",
        "    print(f\"MLflow not available: {_e}\")\n",
        "\n",
        "class _ML:\n",
        "    @staticmethod\n",
        "    def start_run(run_name):\n",
        "        if _mlflow_available:\n",
        "            return mlflow.start_run(run_name=run_name)\n",
        "        class _Noop:\n",
        "            def __enter__(self):\n",
        "                return type(\"obj\", (), {\"info\": type(\"obj\", (), {\"run_id\": \"NO_MLFLOW\"})})()\n",
        "            def __exit__(self, *args): return False\n",
        "        return _Noop()\n",
        "\n",
        "    @staticmethod\n",
        "    def log_metric(k, v):\n",
        "        if _mlflow_available:\n",
        "            try: mlflow.log_metric(k, float(v))\n",
        "            except Exception as e: print(f\"Warning: failed to log metric {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_param(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                if isinstance(v, (dict, list, tuple)):\n",
        "                    v = json.dumps(v, ensure_ascii=False)\n",
        "                mlflow.log_param(k, str(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: failed to log param {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_artifact(path):\n",
        "        if _mlflow_available and os.path.exists(path):\n",
        "            try: mlflow.log_artifact(path)\n",
        "            except Exception as e: print(f\"Warning: failed to log artifact {path}: {e}\")\n",
        "\n",
        "\n",
        "def monitor_xgboost_production_readiness(\n",
        "    model,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.Series,\n",
        "    high_conf_threshold: float = 0.80,\n",
        "    balance_target: float = 0.50\n",
        "):\n",
        "    \"\"\"\n",
        "    Monitor an already-trained XGBoost (or sklearn-compatible) classifier for production readiness.\n",
        "    Handles binary and multiclass. Calibration curve is computed only for binary with predict_proba.\n",
        "    \"\"\"\n",
        "\n",
        "    with _ML.start_run(run_name=\"production_readiness_monitoring\"):\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Probabilities (best-effort)\n",
        "        proba = None\n",
        "        try:\n",
        "            proba = model.predict_proba(X_test)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Determine classes and binary/multiclass\n",
        "        classes = getattr(model, \"classes_\", np.unique(y_test))\n",
        "        binary = len(classes) == 2\n",
        "\n",
        "        # Choose a positive label consistently (prefer 1 if present)\n",
        "        pos_label = 1 if binary and (1 in classes) else (classes.max() if binary else None)\n",
        "\n",
        "        # Confidence handling\n",
        "        avg_conf = np.nan\n",
        "        conf_var = np.nan\n",
        "        high_conf_rate = np.nan\n",
        "\n",
        "        if proba is not None:\n",
        "            if binary:\n",
        "                class_to_col = {c: i for i, c in enumerate(classes)}\n",
        "                pos_idx = class_to_col[pos_label]\n",
        "                y_pred_proba = proba[:, pos_idx]\n",
        "                avg_conf = float(np.mean(y_pred_proba))\n",
        "                conf_var = float(np.var(y_pred_proba))\n",
        "                high_conf_rate = float(np.mean((y_pred_proba > high_conf_threshold) | (y_pred_proba < (1 - high_conf_threshold))))\n",
        "            else:\n",
        "                # For multiclass, use the max class probability as confidence\n",
        "                max_conf = proba.max(axis=1)\n",
        "                avg_conf = float(np.mean(max_conf))\n",
        "                conf_var = float(np.var(max_conf))\n",
        "                high_conf_rate = float(np.mean(max_conf > high_conf_threshold))\n",
        "\n",
        "        # Positive prediction rate and variance\n",
        "        if binary:\n",
        "            pred_pos = (y_pred == pos_label).astype(float)\n",
        "            positive_rate = float(pred_pos.mean())\n",
        "            prediction_variance = float(pred_pos.var())\n",
        "            prediction_balance = float(abs(positive_rate - balance_target))\n",
        "        else:\n",
        "            # For multiclass, define \"positive rate\" as proportion of the most frequent predicted class\n",
        "            _, counts = np.unique(y_pred, return_counts=True)\n",
        "            positive_rate = float(counts.max() / len(y_pred))\n",
        "            # Variance of predicted class indices as a simple dispersion proxy\n",
        "            # (still loggable, though less interpretable than binary case)\n",
        "            class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "            pred_idx = np.vectorize(class_to_idx.get)(y_pred)\n",
        "            prediction_variance = float(np.var(pred_idx))\n",
        "            # Balance relative to uniform distribution across classes\n",
        "            uniform = 1.0 / len(classes)\n",
        "            prediction_balance = float(abs(positive_rate - uniform))\n",
        "\n",
        "        # Aggregate metrics\n",
        "        metrics = {\n",
        "            \"positive_prediction_rate\": positive_rate,\n",
        "            \"prediction_variance\": prediction_variance,\n",
        "            \"avg_prediction_confidence\": avg_conf,\n",
        "            \"confidence_variance\": conf_var,\n",
        "            \"high_confidence_rate\": high_conf_rate,\n",
        "            \"prediction_balance\": prediction_balance\n",
        "        }\n",
        "\n",
        "        # Log metrics\n",
        "        for k, v in metrics.items():\n",
        "            if not (isinstance(v, float) and (np.isnan(v) or np.isinf(v))):\n",
        "                _ML.log_metric(k, v)\n",
        "\n",
        "        # Feature importance\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            fi_df = pd.DataFrame({\n",
        "                \"feature\": X_test.columns,\n",
        "                \"importance\": model.feature_importances_\n",
        "            }).sort_values(\"importance\", ascending=False)\n",
        "            top_features = fi_df.head(10)[\"feature\"].tolist()\n",
        "            _ML.log_param(\"top_10_features\", top_features)\n",
        "            if not fi_df.empty:\n",
        "                _ML.log_metric(\"top_feature_dominance\", float(fi_df.iloc[0][\"importance\"]))\n",
        "\n",
        "        # Stability checks (simple heuristics)\n",
        "        if binary:\n",
        "            balanced_predictions = metrics[\"prediction_balance\"] < 0.30\n",
        "        else:\n",
        "            # For multiclass, require no class to dominate excessively\n",
        "            balanced_predictions = positive_rate < 0.70\n",
        "\n",
        "        confident_predictions = (not np.isnan(high_conf_rate)) and (high_conf_rate > 0.70)\n",
        "        stable_variance = metrics[\"prediction_variance\"] < (0.30 if binary else 1.00)\n",
        "        reasonable_confidence = (not np.isnan(avg_conf)) and (0.30 < avg_conf < 0.70)\n",
        "\n",
        "        stability_checks = {\n",
        "            \"balanced_predictions\": balanced_predictions,\n",
        "            \"confident_predictions\": confident_predictions,\n",
        "            \"stable_variance\": stable_variance,\n",
        "            \"reasonable_confidence\": reasonable_confidence\n",
        "        }\n",
        "\n",
        "        stability_score = sum(bool(v) for v in stability_checks.values()) / len(stability_checks)\n",
        "        _ML.log_metric(\"stability_score\", stability_score)\n",
        "\n",
        "        print(\"Production Readiness Monitoring Results:\")\n",
        "        print(f\"  Positive prediction rate: {metrics['positive_prediction_rate']:.3f}\")\n",
        "        if not np.isnan(avg_conf):\n",
        "            print(f\"  Average confidence: {metrics['avg_prediction_confidence']:.3f}\")\n",
        "        if not np.isnan(high_conf_rate):\n",
        "            print(f\"  High confidence rate: {metrics['high_confidence_rate']:.3f}\")\n",
        "        print(f\"  Stability score: {stability_score:.3f}\")\n",
        "\n",
        "        for check, passed in stability_checks.items():\n",
        "            status = \"PASS\" if passed else \"FAIL\"\n",
        "            print(f\"  {status} - {check.replace('_', ' ').title()}: {passed}\")\n",
        "\n",
        "        # Calibration curve (binary only with probabilities)\n",
        "        calibration_error = np.nan\n",
        "        if binary and proba is not None:\n",
        "            class_to_col = {c: i for i, c in enumerate(classes)}\n",
        "            pos_idx = class_to_col[pos_label]\n",
        "            y_pred_proba = proba[:, pos_idx]\n",
        "\n",
        "            frac_pos, mean_pred = calibration_curve(y_test, y_pred_proba, n_bins=10, strategy=\"uniform\")\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(mean_pred, frac_pos, \"s-\", label=\"Model\", linewidth=2, markersize=6)\n",
        "            plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\", linewidth=1)\n",
        "            plt.xlabel(\"Mean Predicted Probability\")\n",
        "            plt.ylabel(\"Fraction of Positives\")\n",
        "            plt.title(\"Model Calibration\")\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "\n",
        "            os.makedirs(\"plots\", exist_ok=True)\n",
        "            cal_path = \"plots/production_calibration_xgboost.png\"\n",
        "            plt.savefig(cal_path, dpi=300, bbox_inches=\"tight\")\n",
        "            _ML.log_artifact(cal_path)\n",
        "            plt.show()\n",
        "\n",
        "            calibration_error = float(np.mean(np.abs(frac_pos - mean_pred)))\n",
        "            _ML.log_metric(\"calibration_error\", calibration_error)\n",
        "            print(f\"  Calibration error: {calibration_error:.4f}\")\n",
        "        else:\n",
        "            print(\"  Calibration skipped (requires binary classification with predict_proba).\")\n",
        "\n",
        "        # Final production readiness flag\n",
        "        production_ready = (stability_score > 0.70) and (np.isnan(calibration_error) or calibration_error < 0.10)\n",
        "        _ML.log_param(\"production_ready\", production_ready)\n",
        "        print(f\"\\nProduction readiness: {production_ready}\")\n",
        "\n",
        "        return {\n",
        "            \"stability_score\": stability_score,\n",
        "            \"calibration_error\": calibration_error,\n",
        "            \"production_ready\": production_ready\n",
        "        }\n",
        "\n",
        "# Run production readiness monitoring\n",
        "print(\"Starting production readiness monitoring...\")\n",
        "production_metrics = monitor_xgboost_production_readiness(xgb_model, X_test, y_test)\n",
        "print(\"Production readiness monitoring completed.\")\n"
      ],
      "id": "tYd0_09DEAeP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d6ef71",
      "metadata": {
        "id": "03d6ef71"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Optional MLflow wrappers ----------\n",
        "_mlflow_available = False\n",
        "try:\n",
        "    import mlflow\n",
        "    _mlflow_available = True\n",
        "except Exception as _e:\n",
        "    print(f\"MLflow not available: {_e}\")\n",
        "\n",
        "class _ML:\n",
        "    @staticmethod\n",
        "    def start_run(run_name):\n",
        "        if _mlflow_available:\n",
        "            return mlflow.start_run(run_name=run_name)\n",
        "        class _Noop:\n",
        "            def __enter__(self):\n",
        "                return type(\"obj\", (), {\"info\": type(\"obj\", (), {\"run_id\": \"NO_MLFLOW\"})})()\n",
        "            def __exit__(self, *args): return False\n",
        "        return _Noop()\n",
        "\n",
        "    @staticmethod\n",
        "    def log_metric(k, v):\n",
        "        if _mlflow_available:\n",
        "            try: mlflow.log_metric(k, float(v))\n",
        "            except Exception as e: print(f\"Warning: failed to log metric {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_param(k, v):\n",
        "        if _mlflow_available:\n",
        "            try:\n",
        "                if isinstance(v, (dict, list, tuple)):\n",
        "                    v = json.dumps(v, ensure_ascii=False)\n",
        "                mlflow.log_param(k, str(v))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: failed to log param {k}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def log_artifact(path):\n",
        "        if _mlflow_available and os.path.exists(path):\n",
        "            try: mlflow.log_artifact(path)\n",
        "            except Exception as e: print(f\"Warning: failed to log artifact {path}: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_tracking_uri():\n",
        "        if _mlflow_available:\n",
        "            try: return mlflow.get_tracking_uri()\n",
        "            except Exception: return \"UNKNOWN\"\n",
        "        return \"NO_MLFLOW\"\n",
        "\n",
        "\n",
        "def _get_metric(mdict, *keys, default=None):\n",
        "    \"\"\"Fetch a metric from dict using any of several key aliases (case-insensitive).\"\"\"\n",
        "    if not isinstance(mdict, dict):\n",
        "        return default\n",
        "    lower_map = {str(k).lower(): v for k, v in mdict.items()}\n",
        "    for k in keys:\n",
        "        if str(k).lower() in lower_map:\n",
        "            return lower_map[str(k).lower()]\n",
        "    return default\n",
        "\n",
        "\n",
        "print(\"Generating streamlined XGBoost MLOps pipeline summary...\")\n",
        "\n",
        "with _ML.start_run(run_name=\"streamlined_xgboost_pipeline_summary\"):\n",
        "    # Pull metrics with robust key handling (supports both 'Accuracy' and 'accuracy')\n",
        "    acc = _get_metric(final_metrics, \"Accuracy\", \"accuracy\")\n",
        "    f1  = _get_metric(final_metrics, \"F1 Score\", \"f1_score\", \"f1\")\n",
        "    auc = _get_metric(final_metrics, \"AUC\", \"auc\")\n",
        "    prec = _get_metric(final_metrics, \"Precision\", \"precision\")\n",
        "    rec  = _get_metric(final_metrics, \"Recall\", \"recall\")\n",
        "\n",
        "    # Build pipeline summary dict\n",
        "    pipeline_summary = {\n",
        "        \"model_type\": \"XGBoost_Only\",\n",
        "        \"pipeline_approach\": \"Streamlined_Single_Model\",\n",
        "        \"final_accuracy\": acc,\n",
        "        \"final_f1_score\": f1,\n",
        "        \"final_auc\": auc,\n",
        "        \"final_precision\": prec,\n",
        "        \"final_recall\": rec,\n",
        "        \"data_source\": data_source if \"data_source\" in globals() else \"UNKNOWN\",\n",
        "        \"feature_count\": int(X.shape[1]) if \"X\" in globals() else -1,\n",
        "        \"training_samples\": int(len(X_train)) if \"X_train\" in globals() else -1,\n",
        "        \"test_samples\": int(len(X_test)) if \"X_test\" in globals() else -1,\n",
        "        \"stability_score\": float(production_metrics.get(\"stability_score\", np.nan)) if isinstance(production_metrics, dict) else np.nan,\n",
        "        \"calibration_error\": float(production_metrics.get(\"calibration_error\", np.nan)) if isinstance(production_metrics, dict) else np.nan,\n",
        "        \"production_ready\": bool(production_metrics.get(\"production_ready\", False)) if isinstance(production_metrics, dict) else False,\n",
        "        \"pipeline_type\": \"streamlined_xgboost_mlops\"\n",
        "    }\n",
        "\n",
        "    # Log all metrics/params\n",
        "    for key, value in pipeline_summary.items():\n",
        "        if isinstance(value, (int, float)) and not (isinstance(value, float) and (np.isnan(value) or np.isinf(value))):\n",
        "            _ML.log_metric(key, value)\n",
        "        else:\n",
        "            _ML.log_param(key, value)\n",
        "\n",
        "    # Prepare performance metrics for plotting (only include those available)\n",
        "    metrics_data = {}\n",
        "    if acc is not None:  metrics_data[\"Accuracy\"] = acc\n",
        "    if prec is not None: metrics_data[\"Precision\"] = prec\n",
        "    if rec is not None:  metrics_data[\"Recall\"] = rec\n",
        "    if f1 is not None:   metrics_data[\"F1 Score\"] = f1\n",
        "    if auc is not None:  metrics_data[\"AUC\"] = auc  # may be None for some multiclass setups\n",
        "\n",
        "    # Create plots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # --- Metrics bar chart ---\n",
        "    if metrics_data:\n",
        "        bars = ax1.bar(list(metrics_data.keys()), list(metrics_data.values()), alpha=0.85)\n",
        "        ax1.set_title(\"XGBoost Model Performance Metrics\", fontsize=14, fontweight=\"bold\")\n",
        "        ax1.set_ylabel(\"Score\", fontsize=12)\n",
        "        ax1.set_ylim(0, 1.0)\n",
        "        ax1.grid(axis=\"y\", alpha=0.3)\n",
        "        for bar, value in zip(bars, metrics_data.values()):\n",
        "            ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "                     f\"{value:.3f}\", ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=10)\n",
        "    else:\n",
        "        ax1.set_title(\"XGBoost Model Performance Metrics (no metrics available)\", fontsize=14)\n",
        "\n",
        "    # --- Production readiness gauge-like bars ---\n",
        "    # For calibration quality, if calibration_error is NaN, treat quality as NaN too\n",
        "    cal_err = pipeline_summary[\"calibration_error\"]\n",
        "    cal_quality = (1 - cal_err) if isinstance(cal_err, (int, float)) and not np.isnan(cal_err) else np.nan\n",
        "    overall_perf = float(np.nanmean(list(metrics_data.values()))) if metrics_data else np.nan\n",
        "\n",
        "    readiness_data = {\n",
        "        \"Stability Score\": pipeline_summary[\"stability_score\"],\n",
        "        \"Calibration Quality\": cal_quality,\n",
        "        \"Overall Performance\": overall_perf\n",
        "    }\n",
        "\n",
        "    # Assign colours by threshold (green/yellow/red) when value is valid; fall back to grey for NaN\n",
        "    def colour_for(v):\n",
        "        if isinstance(v, (int, float)) and not np.isnan(v):\n",
        "            if v > 0.8: return \"#28a745\"\n",
        "            if v > 0.6: return \"#ffc107\"\n",
        "            return \"#dc3545\"\n",
        "        return \"#6c757d\"\n",
        "\n",
        "    colors = [colour_for(v) for v in readiness_data.values()]\n",
        "    bars2 = ax2.bar(list(readiness_data.keys()), list(readiness_data.values()), color=colors, alpha=0.85)\n",
        "    ax2.set_title(\"Production Readiness Assessment\", fontsize=14, fontweight=\"bold\")\n",
        "    ax2.set_ylabel(\"Score\", fontsize=12)\n",
        "    ax2.set_ylim(0, 1.0)\n",
        "    ax2.grid(axis=\"y\", alpha=0.3)\n",
        "    for bar, value in zip(bars2, readiness_data.values()):\n",
        "        if isinstance(value, (int, float)) and not np.isnan(value):\n",
        "            ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "                     f\"{value:.3f}\", ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=10)\n",
        "        else:\n",
        "            ax2.text(bar.get_x() + bar.get_width() / 2, 0.02, \"N/A\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save summary plot\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    summary_path = \"plots/streamlined_xgboost_summary.png\"\n",
        "    plt.savefig(summary_path, dpi=300, bbox_inches=\"tight\")\n",
        "    _ML.log_artifact(summary_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Save CSV artifacts\n",
        "    try:\n",
        "        if metrics_data:\n",
        "            pd.DataFrame([metrics_data]).to_csv(\"results/performance_metrics.csv\", index=False)\n",
        "            _ML.log_artifact(\"results/performance_metrics.csv\")\n",
        "\n",
        "        summary_df = pd.DataFrame([pipeline_summary])\n",
        "        summary_df.to_csv(\"results/pipeline_summary.csv\", index=False)\n",
        "        _ML.log_artifact(\"results/pipeline_summary.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save results: {e}\")\n",
        "\n",
        "    # Final summary printout\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STREAMLINED MUSHROOM CLASSIFICATION XGBOOST PIPELINE COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Model: XGBoost (Single Model Approach)\")\n",
        "    if acc is not None: print(f\"Accuracy: {acc:.4f}\")\n",
        "    if f1  is not None: print(f\"F1 Score: {f1:.4f}\")\n",
        "    if auc is not None: print(f\"AUC: {auc:.4f}\")\n",
        "    if prec is not None: print(f\"Precision: {prec:.4f}\")\n",
        "    if rec  is not None: print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Stability Score: {pipeline_summary['stability_score']}\")\n",
        "    print(f\"Calibration Error: {pipeline_summary['calibration_error']}\")\n",
        "    print(f\"Production Ready: {pipeline_summary['production_ready']}\")\n",
        "    print(f\"MLflow URI: {_ML.get_tracking_uri()}\")\n",
        "    print(f\"Results: plots/ and results/ directories\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"No A/B testing required - single model deployment target.\")\n",
        "    print(\"Focused, efficient, and production-ready pipeline.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(\"Streamlined XGBoost MLOps pipeline completed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}